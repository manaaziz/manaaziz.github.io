---
layout: post
title: Fourth Blog Post
---

For the second project for ST558, we had to perform an analysis in R on an online news popularity data set, which came from Mashable. We then had to go ahead and automate RMarkdown to perform the analysis for every day of the week. 

I started the project by **thoroughly** reading through the directions and getting as much information as I could on the data. I came across a bunch of articles that used the data as well as the original study that the data had come from. I used those to my advantage later in the variable selection phase, since there was no specific instruction on *how* exactly we had to come up with the variables, just to make sure we explained the variables that we used. After getting all the variables that I was going to use, I went ahead and did the summaries and fit the models and compared the RMSE, basically just following what we saw in the supervised learning lectures.

This project was definitely much more painful than the first. Not because of the level of difficulty, the length of the assignment, or anything that actually relates to the project itself, but because of the fact that I do all of my work on an iMac from 2010. Every time I wanted to run the code for the Monday report, it took 10 minutes (yes, I actually timed it). Then, when I wanted to run the code to spit out all of the reports, you guessed it... over an hour of wait time. Luckily I had 3 other classes' worth of work to do so I could just do other homework while I waited. That was the most difficult part for me by far. That and figuring out how to un-break github when I deleted some files. If I could do one thing differently, it would probably be get a better computer lol.

All that being said, check out my repo page [here](https://manaaziz.github.io/ST558-RProj2/)

